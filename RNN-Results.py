Python 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license()" for more information.
>>> 
======== RESTART: C:\Users\catcry\Desktop\Pattern\Project\pro\RNN.py ========
Using TensorFlow backend.
Train on 113375 samples, validate on 12598 samples
Epoch 1/150
 - 3s - loss: 0.4553 - val_loss: 0.4197
Epoch 2/150
 - 3s - loss: 0.3877 - val_loss: 0.3493
Epoch 3/150
 - 3s - loss: 0.3244 - val_loss: 0.2939
Epoch 4/150
 - 3s - loss: 0.2760 - val_loss: 0.2544
Epoch 5/150
 - 3s - loss: 0.2427 - val_loss: 0.2283
Epoch 6/150
 - 3s - loss: 0.2208 - val_loss: 0.2110
Epoch 7/150
 - 3s - loss: 0.2062 - val_loss: 0.1991
Epoch 8/150
 - 3s - loss: 0.1959 - val_loss: 0.1906
Epoch 9/150
 - 3s - loss: 0.1883 - val_loss: 0.1841
Epoch 10/150
 - 3s - loss: 0.1824 - val_loss: 0.1788
Epoch 11/150
 - 3s - loss: 0.1775 - val_loss: 0.1744
Epoch 12/150
 - 3s - loss: 0.1734 - val_loss: 0.1706
Epoch 13/150
 - 3s - loss: 0.1698 - val_loss: 0.1672
Epoch 14/150
 - 3s - loss: 0.1665 - val_loss: 0.1641
Epoch 15/150
 - 3s - loss: 0.1634 - val_loss: 0.1610
Epoch 16/150
 - 3s - loss: 0.1604 - val_loss: 0.1581
Epoch 17/150
 - 3s - loss: 0.1576 - val_loss: 0.1553
Epoch 18/150
 - 3s - loss: 0.1549 - val_loss: 0.1528
Epoch 19/150
 - 3s - loss: 0.1524 - val_loss: 0.1503
Epoch 20/150
 - 3s - loss: 0.1501 - val_loss: 0.1481
Epoch 21/150
 - 3s - loss: 0.1478 - val_loss: 0.1459
Epoch 22/150
 - 3s - loss: 0.1456 - val_loss: 0.1437
Epoch 23/150
 - 3s - loss: 0.1434 - val_loss: 0.1415
Epoch 24/150
 - 3s - loss: 0.1413 - val_loss: 0.1394
Epoch 25/150
 - 3s - loss: 0.1391 - val_loss: 0.1373
Epoch 26/150
 - 3s - loss: 0.1370 - val_loss: 0.1352
Epoch 27/150
 - 3s - loss: 0.1349 - val_loss: 0.1332
Epoch 28/150
 - 3s - loss: 0.1329 - val_loss: 0.1313
Epoch 29/150
 - 3s - loss: 0.1311 - val_loss: 0.1298
Epoch 30/150
 - 3s - loss: 0.1296 - val_loss: 0.1285
Epoch 31/150
 - 3s - loss: 0.1284 - val_loss: 0.1274
Epoch 32/150
 - 3s - loss: 0.1274 - val_loss: 0.1264
Epoch 33/150
 - 3s - loss: 0.1264 - val_loss: 0.1255
Epoch 34/150
 - 3s - loss: 0.1255 - val_loss: 0.1247
Epoch 35/150
 - 3s - loss: 0.1247 - val_loss: 0.1239
Epoch 36/150
 - 3s - loss: 0.1240 - val_loss: 0.1231
Epoch 37/150
 - 3s - loss: 0.1233 - val_loss: 0.1225
Epoch 38/150
 - 3s - loss: 0.1226 - val_loss: 0.1218
Epoch 39/150
 - 3s - loss: 0.1220 - val_loss: 0.1212
Epoch 40/150
 - 3s - loss: 0.1214 - val_loss: 0.1206
Epoch 41/150
 - 3s - loss: 0.1208 - val_loss: 0.1201
Epoch 42/150
 - 3s - loss: 0.1203 - val_loss: 0.1196
Epoch 43/150
 - 3s - loss: 0.1198 - val_loss: 0.1191
Epoch 44/150
 - 3s - loss: 0.1193 - val_loss: 0.1186
Epoch 45/150
 - 3s - loss: 0.1188 - val_loss: 0.1181
Epoch 46/150
 - 3s - loss: 0.1184 - val_loss: 0.1177
Epoch 47/150
 - 3s - loss: 0.1179 - val_loss: 0.1173
Epoch 48/150
 - 3s - loss: 0.1175 - val_loss: 0.1169
Epoch 49/150
 - 3s - loss: 0.1171 - val_loss: 0.1165
Epoch 50/150
 - 3s - loss: 0.1167 - val_loss: 0.1161
Epoch 51/150
 - 3s - loss: 0.1164 - val_loss: 0.1157
Epoch 52/150
 - 3s - loss: 0.1160 - val_loss: 0.1154
Epoch 53/150
 - 3s - loss: 0.1157 - val_loss: 0.1150
Epoch 54/150
 - 3s - loss: 0.1153 - val_loss: 0.1147
Epoch 55/150
 - 3s - loss: 0.1150 - val_loss: 0.1144
Epoch 56/150
 - 3s - loss: 0.1147 - val_loss: 0.1140
Epoch 57/150
 - 3s - loss: 0.1143 - val_loss: 0.1137
Epoch 58/150
 - 3s - loss: 0.1140 - val_loss: 0.1134
Epoch 59/150
 - 3s - loss: 0.1137 - val_loss: 0.1131
Epoch 60/150
 - 3s - loss: 0.1134 - val_loss: 0.1128
Epoch 61/150
 - 3s - loss: 0.1131 - val_loss: 0.1126
Epoch 62/150
 - 3s - loss: 0.1129 - val_loss: 0.1123
Epoch 63/150
 - 3s - loss: 0.1126 - val_loss: 0.1120
Epoch 64/150
 - 3s - loss: 0.1123 - val_loss: 0.1117
Epoch 65/150
 - 3s - loss: 0.1120 - val_loss: 0.1115
Epoch 66/150
 - 3s - loss: 0.1118 - val_loss: 0.1112
Epoch 67/150
 - 3s - loss: 0.1115 - val_loss: 0.1110
Epoch 68/150
 - 3s - loss: 0.1113 - val_loss: 0.1107
Epoch 69/150
 - 3s - loss: 0.1110 - val_loss: 0.1105
Epoch 70/150
 - 3s - loss: 0.1108 - val_loss: 0.1102
Epoch 71/150
 - 3s - loss: 0.1105 - val_loss: 0.1100
Epoch 72/150
 - 3s - loss: 0.1103 - val_loss: 0.1098
Epoch 73/150
 - 3s - loss: 0.1101 - val_loss: 0.1096
Epoch 74/150
 - 3s - loss: 0.1098 - val_loss: 0.1093
Epoch 75/150
 - 3s - loss: 0.1096 - val_loss: 0.1091
Epoch 76/150
 - 3s - loss: 0.1094 - val_loss: 0.1089
Epoch 77/150
 - 3s - loss: 0.1092 - val_loss: 0.1087
Epoch 78/150
 - 3s - loss: 0.1090 - val_loss: 0.1085
Epoch 79/150
 - 3s - loss: 0.1087 - val_loss: 0.1083
Epoch 80/150
 - 3s - loss: 0.1085 - val_loss: 0.1081
Epoch 81/150
 - 3s - loss: 0.1083 - val_loss: 0.1079
Epoch 82/150
 - 3s - loss: 0.1081 - val_loss: 0.1077
Epoch 83/150
 - 3s - loss: 0.1079 - val_loss: 0.1075
Epoch 84/150
 - 3s - loss: 0.1077 - val_loss: 0.1073
Epoch 85/150
 - 3s - loss: 0.1075 - val_loss: 0.1071
Epoch 86/150
 - 3s - loss: 0.1073 - val_loss: 0.1069
Epoch 87/150
 - 3s - loss: 0.1071 - val_loss: 0.1067
Epoch 88/150
 - 3s - loss: 0.1069 - val_loss: 0.1066
Epoch 89/150
 - 3s - loss: 0.1068 - val_loss: 0.1064
Epoch 90/150
 - 3s - loss: 0.1066 - val_loss: 0.1062
Epoch 91/150
 - 3s - loss: 0.1064 - val_loss: 0.1061
Epoch 92/150
 - 3s - loss: 0.1062 - val_loss: 0.1059
Epoch 93/150
 - 3s - loss: 0.1061 - val_loss: 0.1057
Epoch 94/150
 - 3s - loss: 0.1059 - val_loss: 0.1056
Epoch 95/150
 - 3s - loss: 0.1058 - val_loss: 0.1054
Epoch 96/150
 - 3s - loss: 0.1056 - val_loss: 0.1053
Epoch 97/150
 - 3s - loss: 0.1055 - val_loss: 0.1052
Epoch 98/150
 - 3s - loss: 0.1053 - val_loss: 0.1050
Epoch 99/150
 - 3s - loss: 0.1052 - val_loss: 0.1049
Epoch 100/150
 - 3s - loss: 0.1050 - val_loss: 0.1048
Epoch 101/150
 - 3s - loss: 0.1049 - val_loss: 0.1047
Epoch 102/150
 - 3s - loss: 0.1048 - val_loss: 0.1046
Epoch 103/150
 - 3s - loss: 0.1047 - val_loss: 0.1045
Epoch 104/150
 - 3s - loss: 0.1046 - val_loss: 0.1044
Epoch 105/150
 - 3s - loss: 0.1045 - val_loss: 0.1043
Epoch 106/150
 - 3s - loss: 0.1044 - val_loss: 0.1042
Epoch 107/150
 - 3s - loss: 0.1043 - val_loss: 0.1041
Epoch 108/150
 - 3s - loss: 0.1042 - val_loss: 0.1041
Epoch 109/150
 - 3s - loss: 0.1041 - val_loss: 0.1040
Epoch 110/150
 - 3s - loss: 0.1040 - val_loss: 0.1039
Epoch 111/150
 - 3s - loss: 0.1039 - val_loss: 0.1038
Epoch 112/150
 - 3s - loss: 0.1038 - val_loss: 0.1037
Epoch 113/150
 - 3s - loss: 0.1037 - val_loss: 0.1036
Epoch 114/150
 - 3s - loss: 0.1036 - val_loss: 0.1035
Epoch 115/150
 - 3s - loss: 0.1035 - val_loss: 0.1034
Epoch 116/150
 - 3s - loss: 0.1034 - val_loss: 0.1033
Epoch 117/150
 - 3s - loss: 0.1033 - val_loss: 0.1033
Epoch 118/150
 - 3s - loss: 0.1033 - val_loss: 0.1032
Epoch 119/150
 - 3s - loss: 0.1032 - val_loss: 0.1031
Epoch 120/150
 - 3s - loss: 0.1031 - val_loss: 0.1030
Epoch 121/150
 - 3s - loss: 0.1030 - val_loss: 0.1029
Epoch 122/150
 - 3s - loss: 0.1029 - val_loss: 0.1028
Epoch 123/150
 - 3s - loss: 0.1028 - val_loss: 0.1027
Epoch 124/150
 - 3s - loss: 0.1028 - val_loss: 0.1027
Epoch 125/150
 - 3s - loss: 0.1027 - val_loss: 0.1026
Epoch 126/150
 - 3s - loss: 0.1026 - val_loss: 0.1025
Epoch 127/150
 - 3s - loss: 0.1025 - val_loss: 0.1024
Epoch 128/150
 - 3s - loss: 0.1025 - val_loss: 0.1024
Epoch 129/150
 - 3s - loss: 0.1024 - val_loss: 0.1023
Epoch 130/150
 - 3s - loss: 0.1023 - val_loss: 0.1023
Epoch 131/150
 - 3s - loss: 0.1022 - val_loss: 0.1022
Epoch 132/150
 - 3s - loss: 0.1022 - val_loss: 0.1022
Epoch 133/150
 - 3s - loss: 0.1021 - val_loss: 0.1021
Epoch 134/150
 - 3s - loss: 0.1020 - val_loss: 0.1021
Epoch 135/150
 - 3s - loss: 0.1019 - val_loss: 0.1020
Epoch 136/150
 - 3s - loss: 0.1019 - val_loss: 0.1019
Epoch 137/150
 - 3s - loss: 0.1018 - val_loss: 0.1019
Epoch 138/150
 - 3s - loss: 0.1017 - val_loss: 0.1018
Epoch 139/150
 - 3s - loss: 0.1017 - val_loss: 0.1018
Epoch 140/150
 - 3s - loss: 0.1016 - val_loss: 0.1017
Epoch 141/150
 - 3s - loss: 0.1015 - val_loss: 0.1016
Epoch 142/150
 - 3s - loss: 0.1015 - val_loss: 0.1016
Epoch 143/150
 - 3s - loss: 0.1014 - val_loss: 0.1015
Epoch 144/150
 - 3s - loss: 0.1013 - val_loss: 0.1014
Epoch 145/150
 - 3s - loss: 0.1013 - val_loss: 0.1014
Epoch 146/150
 - 3s - loss: 0.1012 - val_loss: 0.1013
Epoch 147/150
 - 3s - loss: 0.1011 - val_loss: 0.1013
Epoch 148/150
 - 3s - loss: 0.1011 - val_loss: 0.1012
Epoch 149/150
 - 3s - loss: 0.1010 - val_loss: 0.1011
Epoch 150/150
 - 3s - loss: 0.1010 - val_loss: 0.1011
======================Results of XTrain Set (90% of KDDTrain+ Set): ===================================

The Specification of Classifier is: model.fit (Xtrain,Ytrain, epochs = 100, batch_size=10000, validation_data=(Xvalid,Yvalid), verbose=2, shuffle=False)
model Loss = mae , Optimizer = sgd
Number of un-Detected Attacks (FN) in Training Set is: FN =  3554  out of 113375
Number of False Attack Alarms (FP) in Training Set is: FP = 5551  out of 113375
Accuracy on Trainin Set: Accuracy =  0.919691289966924 %

======================Results of KDDTrain+ Set: ===================================

The Specification of Classifier is: model.fit (Xtrain,Ytrain, epochs = 100, batch_size=10000, validation_data=(Xvalid,Yvalid), verbose=2, shuffle=False)
Number of un-Detected Attacks (FN) in Training Set is: FN =  3959  out of 125973
Number of False Attack Alarms (FP) in Training Set is: FP = 6144  out of 125973
Accuracy on Trainin Set: Accuracy =  0.9198002746620307 %

======================Results of KDDTest+ Set: ===================================

The Specification of Classifier is: <keras.callbacks.History object at 0x00000120E6758C50>
Number of un-Detected Attacks (FN) in Training Set is: FN =  3274  out of 22543
Number of False Attack Alarms (FP) in Training Set is: FP = 289  out of 22543
Accuracy on Trainin Set: Accuracy =  0.8419465022401632 %

======================Results of KDDTest-21 Set: ===================================

The Specification of Classifier is: <keras.callbacks.History object at 0x00000120E6758C50>
Number of un-Detected Attacks (FN) in Training Set is: FN =  3281  out of 11850
Number of False Attack Alarms (FP) in Training Set is: FP = 240  out of 11850
Accuracy on Trainin Set: Accuracy =  0.7028691983122363 %

>>> 
======== RESTART: C:\Users\catcry\Desktop\Pattern\Project\pro\RNN.py ========
Using TensorFlow backend.
Train on 113375 samples, validate on 12598 samples
Epoch 1/150
 - 5s - loss: 0.4537 - val_loss: 0.4184
Epoch 2/150
 - 5s - loss: 0.3893 - val_loss: 0.3518
Epoch 3/150
 - 5s - loss: 0.3251 - val_loss: 0.2946
Epoch 4/150
 - 5s - loss: 0.2749 - val_loss: 0.2542
Epoch 5/150
 - 5s - loss: 0.2409 - val_loss: 0.2276
Epoch 6/150
 - 5s - loss: 0.2187 - val_loss: 0.2103
Epoch 7/150
 - 4s - loss: 0.2042 - val_loss: 0.1987
Epoch 8/150
 - 5s - loss: 0.1942 - val_loss: 0.1905
Epoch 9/150
 - 5s - loss: 0.1870 - val_loss: 0.1842
Epoch 10/150
 - 5s - loss: 0.1814 - val_loss: 0.1792
Epoch 11/150
 - 5s - loss: 0.1768 - val_loss: 0.1750
Epoch 12/150
 - 5s - loss: 0.1729 - val_loss: 0.1713
Epoch 13/150
 - 5s - loss: 0.1695 - val_loss: 0.1680
Epoch 14/150
 - 5s - loss: 0.1663 - val_loss: 0.1649
Epoch 15/150
 - 5s - loss: 0.1633 - val_loss: 0.1619
Epoch 16/150
 - 5s - loss: 0.1605 - val_loss: 0.1590
Epoch 17/150
 - 5s - loss: 0.1576 - val_loss: 0.1561
Epoch 18/150
 - 5s - loss: 0.1548 - val_loss: 0.1532
Epoch 19/150
 - 5s - loss: 0.1520 - val_loss: 0.1504
Epoch 20/150
 - 5s - loss: 0.1495 - val_loss: 0.1480
Epoch 21/150
 - 5s - loss: 0.1472 - val_loss: 0.1457
Epoch 22/150
 - 4s - loss: 0.1450 - val_loss: 0.1434
Epoch 23/150
 - 4s - loss: 0.1429 - val_loss: 0.1412
Epoch 24/150
 - 4s - loss: 0.1407 - val_loss: 0.1389
Epoch 25/150
 - 4s - loss: 0.1385 - val_loss: 0.1367
Epoch 26/150
 - 5s - loss: 0.1363 - val_loss: 0.1344
Epoch 27/150
 - 4s - loss: 0.1342 - val_loss: 0.1322
Epoch 28/150
 - 5s - loss: 0.1322 - val_loss: 0.1302
Epoch 29/150
 - 5s - loss: 0.1304 - val_loss: 0.1284
Epoch 30/150
 - 5s - loss: 0.1288 - val_loss: 0.1269
Epoch 31/150
 - 5s - loss: 0.1275 - val_loss: 0.1258
Epoch 32/150
 - 5s - loss: 0.1265 - val_loss: 0.1249
Epoch 33/150
 - 4s - loss: 0.1256 - val_loss: 0.1240
Epoch 34/150
 - 5s - loss: 0.1248 - val_loss: 0.1231
Epoch 35/150
 - 5s - loss: 0.1240 - val_loss: 0.1224
Epoch 36/150
 - 4s - loss: 0.1233 - val_loss: 0.1217
Epoch 37/150
 - 4s - loss: 0.1226 - val_loss: 0.1210
Epoch 38/150
 - 5s - loss: 0.1220 - val_loss: 0.1203
Epoch 39/150
 - 5s - loss: 0.1214 - val_loss: 0.1197
Epoch 40/150
 - 5s - loss: 0.1208 - val_loss: 0.1192
Epoch 41/150
 - 5s - loss: 0.1203 - val_loss: 0.1186
Epoch 42/150
 - 5s - loss: 0.1198 - val_loss: 0.1181
Epoch 43/150
 - 4s - loss: 0.1193 - val_loss: 0.1176
Epoch 44/150
 - 4s - loss: 0.1188 - val_loss: 0.1171
Epoch 45/150
 - 4s - loss: 0.1184 - val_loss: 0.1167
Epoch 46/150
 - 4s - loss: 0.1180 - val_loss: 0.1162
Epoch 47/150
 - 5s - loss: 0.1175 - val_loss: 0.1158
Epoch 48/150
 - 5s - loss: 0.1171 - val_loss: 0.1154
Epoch 49/150
 - 5s - loss: 0.1167 - val_loss: 0.1150
Epoch 50/150
 - 4s - loss: 0.1164 - val_loss: 0.1146
Epoch 51/150
 - 5s - loss: 0.1160 - val_loss: 0.1142
Epoch 52/150
 - 5s - loss: 0.1156 - val_loss: 0.1139
Epoch 53/150
 - 5s - loss: 0.1153 - val_loss: 0.1135
Epoch 54/150
 - 5s - loss: 0.1150 - val_loss: 0.1132
Epoch 55/150
 - 4s - loss: 0.1146 - val_loss: 0.1129
Epoch 56/150
 - 4s - loss: 0.1143 - val_loss: 0.1125
Epoch 57/150
 - 4s - loss: 0.1140 - val_loss: 0.1122
Epoch 58/150
 - 4s - loss: 0.1137 - val_loss: 0.1119
Epoch 59/150
 - 4s - loss: 0.1134 - val_loss: 0.1116
Epoch 60/150
 - 5s - loss: 0.1131 - val_loss: 0.1113
Epoch 61/150
 - 4s - loss: 0.1128 - val_loss: 0.1111
Epoch 62/150
 - 5s - loss: 0.1126 - val_loss: 0.1108
Epoch 63/150
 - 5s - loss: 0.1123 - val_loss: 0.1105
Epoch 64/150
 - 5s - loss: 0.1120 - val_loss: 0.1103
Epoch 65/150
 - 5s - loss: 0.1118 - val_loss: 0.1100
Epoch 66/150
 - 5s - loss: 0.1115 - val_loss: 0.1097
Epoch 67/150
 - 5s - loss: 0.1113 - val_loss: 0.1095
Epoch 68/150
 - 5s - loss: 0.1110 - val_loss: 0.1092
Epoch 69/150
 - 5s - loss: 0.1108 - val_loss: 0.1090
Epoch 70/150
 - 4s - loss: 0.1105 - val_loss: 0.1088
Epoch 71/150
 - 4s - loss: 0.1103 - val_loss: 0.1085
Epoch 72/150
 - 5s - loss: 0.1101 - val_loss: 0.1083
Epoch 73/150
 - 5s - loss: 0.1098 - val_loss: 0.1081
Epoch 74/150
 - 5s - loss: 0.1096 - val_loss: 0.1078
Epoch 75/150
 - 4s - loss: 0.1094 - val_loss: 0.1076
Epoch 76/150
 - 4s - loss: 0.1091 - val_loss: 0.1074
Epoch 77/150
 - 4s - loss: 0.1089 - val_loss: 0.1072
Epoch 78/150
 - 4s - loss: 0.1087 - val_loss: 0.1070
Epoch 79/150
 - 5s - loss: 0.1085 - val_loss: 0.1068
Epoch 80/150
 - 5s - loss: 0.1083 - val_loss: 0.1066
Epoch 81/150
 - 4s - loss: 0.1081 - val_loss: 0.1064
Epoch 82/150
 - 4s - loss: 0.1079 - val_loss: 0.1062
Epoch 83/150
 - 5s - loss: 0.1077 - val_loss: 0.1060
Epoch 84/150
 - 5s - loss: 0.1075 - val_loss: 0.1058
Epoch 85/150
 - 4s - loss: 0.1073 - val_loss: 0.1056
Epoch 86/150
 - 5s - loss: 0.1071 - val_loss: 0.1054
Epoch 87/150
 - 5s - loss: 0.1069 - val_loss: 0.1052
Epoch 88/150
 - 5s - loss: 0.1067 - val_loss: 0.1051
Epoch 89/150
 - 5s - loss: 0.1066 - val_loss: 0.1049
Epoch 90/150
 - 4s - loss: 0.1064 - val_loss: 0.1048
Epoch 91/150
 - 5s - loss: 0.1062 - val_loss: 0.1046
Epoch 92/150
 - 5s - loss: 0.1061 - val_loss: 0.1045
Epoch 93/150
 - 5s - loss: 0.1059 - val_loss: 0.1044
Epoch 94/150
 - 5s - loss: 0.1058 - val_loss: 0.1042
Epoch 95/150
 - 5s - loss: 0.1056 - val_loss: 0.1041
Epoch 96/150
 - 5s - loss: 0.1055 - val_loss: 0.1040
Epoch 97/150
 - 5s - loss: 0.1054 - val_loss: 0.1038
Epoch 98/150
 - 5s - loss: 0.1052 - val_loss: 0.1038
Epoch 99/150
 - 4s - loss: 0.1051 - val_loss: 0.1037
Epoch 100/150
 - 5s - loss: 0.1050 - val_loss: 0.1036
Epoch 101/150
 - 5s - loss: 0.1049 - val_loss: 0.1035
Epoch 102/150
 - 5s - loss: 0.1048 - val_loss: 0.1033
Epoch 103/150
 - 5s - loss: 0.1047 - val_loss: 0.1032
Epoch 104/150
 - 5s - loss: 0.1046 - val_loss: 0.1031
Epoch 105/150
 - 5s - loss: 0.1045 - val_loss: 0.1030
Epoch 106/150
 - 5s - loss: 0.1044 - val_loss: 0.1029
Epoch 107/150
 - 5s - loss: 0.1042 - val_loss: 0.1028
Epoch 108/150
 - 5s - loss: 0.1041 - val_loss: 0.1027
Epoch 109/150
 - 5s - loss: 0.1040 - val_loss: 0.1026
Epoch 110/150
 - 5s - loss: 0.1039 - val_loss: 0.1025
Epoch 111/150
 - 5s - loss: 0.1039 - val_loss: 0.1024
Epoch 112/150
 - 5s - loss: 0.1038 - val_loss: 0.1024
Epoch 113/150
 - 5s - loss: 0.1037 - val_loss: 0.1023
Epoch 114/150
 - 5s - loss: 0.1036 - val_loss: 0.1022
Epoch 115/150
 - 5s - loss: 0.1035 - val_loss: 0.1021
Epoch 116/150
 - 5s - loss: 0.1034 - val_loss: 0.1020
Epoch 117/150
 - 5s - loss: 0.1033 - val_loss: 0.1019
Epoch 118/150
 - 5s - loss: 0.1032 - val_loss: 0.1018
Epoch 119/150
 - 5s - loss: 0.1032 - val_loss: 0.1018
Epoch 120/150
 - 5s - loss: 0.1031 - val_loss: 0.1017
Epoch 121/150
 - 5s - loss: 0.1030 - val_loss: 0.1016
Epoch 122/150
 - 5s - loss: 0.1029 - val_loss: 0.1015
Epoch 123/150
 - 5s - loss: 0.1028 - val_loss: 0.1015
Epoch 124/150
 - 5s - loss: 0.1028 - val_loss: 0.1014
Epoch 125/150
 - 5s - loss: 0.1027 - val_loss: 0.1013
Epoch 126/150
 - 5s - loss: 0.1026 - val_loss: 0.1013
Epoch 127/150
 - 5s - loss: 0.1025 - val_loss: 0.1012
Epoch 128/150
 - 5s - loss: 0.1025 - val_loss: 0.1011
Epoch 129/150
 - 5s - loss: 0.1024 - val_loss: 0.1011
Epoch 130/150
 - 5s - loss: 0.1023 - val_loss: 0.1010
Epoch 131/150
 - 5s - loss: 0.1023 - val_loss: 0.1010
Epoch 132/150
 - 5s - loss: 0.1022 - val_loss: 0.1009
Epoch 133/150
 - 5s - loss: 0.1021 - val_loss: 0.1008
Epoch 134/150
 - 5s - loss: 0.1021 - val_loss: 0.1007
Epoch 135/150
 - 5s - loss: 0.1020 - val_loss: 0.1006
Epoch 136/150
 - 5s - loss: 0.1019 - val_loss: 0.1006
Epoch 137/150
 - 5s - loss: 0.1019 - val_loss: 0.1005
Epoch 138/150
 - 5s - loss: 0.1018 - val_loss: 0.1005
Epoch 139/150
 - 5s - loss: 0.1017 - val_loss: 0.1005
Epoch 140/150
 - 5s - loss: 0.1017 - val_loss: 0.1004
Epoch 141/150
 - 5s - loss: 0.1016 - val_loss: 0.1003
Epoch 142/150
 - 5s - loss: 0.1016 - val_loss: 0.1003
Epoch 143/150
 - 5s - loss: 0.1015 - val_loss: 0.1002
Epoch 144/150
 - 5s - loss: 0.1014 - val_loss: 0.1002
Epoch 145/150
 - 5s - loss: 0.1014 - val_loss: 0.1001
Epoch 146/150
 - 5s - loss: 0.1013 - val_loss: 0.1000
Epoch 147/150
 - 5s - loss: 0.1013 - val_loss: 0.1000
Epoch 148/150
 - 5s - loss: 0.1012 - val_loss: 0.1000
Epoch 149/150
 - 5s - loss: 0.1012 - val_loss: 0.0999
Epoch 150/150
 - 4s - loss: 0.1011 - val_loss: 0.0999
======================Results of XTrain Set (90% of KDDTrain+ Set): ===================================

The Specification of Classifier is: model.fit (Xtrain,Ytrain, epochs = 100, batch_size=10000, validation_data=(Xvalid,Yvalid), verbose=2, shuffle=False)
model Loss = mae , Optimizer = sgd
Number of un-Detected Attacks (FN) in Training Set is: FN =  3494  out of 113375
Number of False Attack Alarms (FP) in Training Set is: FP = 5528  out of 113375
Accuracy on Trainin Set: Accuracy =  0.9204233737596472 %

======================Results of KDDTrain+ Set: ===================================

The Specification of Classifier is: model.fit (Xtrain,Ytrain, epochs = 100, batch_size=10000, validation_data=(Xvalid,Yvalid), verbose=2, shuffle=False)
Number of un-Detected Attacks (FN) in Training Set is: FN =  3883  out of 125973
Number of False Attack Alarms (FP) in Training Set is: FP = 6139  out of 125973
Accuracy on Trainin Set: Accuracy =  0.9204432695895152 %

======================Results of KDDTest+ Set: ===================================

The Specification of Classifier is: <keras.callbacks.History object at 0x0000021C09E68C50>
Number of un-Detected Attacks (FN) in Training Set is: FN =  3336  out of 22543
Number of False Attack Alarms (FP) in Training Set is: FP = 299  out of 22543
Accuracy on Trainin Set: Accuracy =  0.8387526061305062 %

======================Results of KDDTest-21 Set: ===================================

The Specification of Classifier is: <keras.callbacks.History object at 0x0000021C09E68C50>
Number of un-Detected Attacks (FN) in Training Set is: FN =  3330  out of 11850
Number of False Attack Alarms (FP) in Training Set is: FP = 247  out of 11850
Accuracy on Trainin Set: Accuracy =  0.6981434599156118 %

>>> 
